{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEUjPdEZII4B",
        "outputId": "c40d6b2e-591a-483a-be7f-812ce8f9307f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 2.6870 - precision: 0.6667 - recall: 0.5679 - val_loss: 5.7162 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 3.0772 - precision: 0.8779 - recall: 0.8357 - val_loss: 1.5938 - val_precision: 0.7259 - val_recall: 0.9333\n",
            "Epoch 3/50\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 1.3851 - precision: 0.9535 - recall: 0.9568 - val_loss: 6.6181 - val_precision: 0.5006 - val_recall: 1.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 2.8990 - precision: 0.9192 - recall: 0.9456 - val_loss: 6.1947 - val_precision: 0.5000 - val_recall: 0.0024\n",
            "Epoch 5/50\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 3.5217 - precision: 0.9500 - recall: 0.9153 - val_loss: 5.2258 - val_precision: 0.5036 - val_recall: 1.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.2850 - precision: 0.9351 - recall: 0.9422 - val_loss: 3.7715 - val_precision: 0.6931 - val_recall: 1.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 3.6024 - precision: 0.9424 - recall: 0.9159 - val_loss: 3.7739 - val_precision: 0.9839 - val_recall: 0.1452\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.65      0.76       420\n",
            "         1.0       0.73      0.93      0.82       420\n",
            "\n",
            "    accuracy                           0.79       840\n",
            "   macro avg       0.82      0.79      0.79       840\n",
            "weighted avg       0.82      0.79      0.79       840\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ✅ Load dataset\n",
        "import pandas as pd\n",
        "data = pd.read_csv('data.csv',on_bad_lines='skip')\n",
        "\n",
        "# Assuming 'data' is your dataframe and 'isFraud' column is what you're using for classification\n",
        "fraud = data[data['isFraud'] == 1]\n",
        "non_fraud = data[data['isFraud'] == 0]\n",
        "nu = list(data['isFraud'].value_counts())[1]\n",
        "\n",
        "#Get same amount of non fraud cases as fraud ones (random selection)\n",
        "non_fraud_sampled = non_fraud.sample(n=nu, random_state=42)\n",
        "\n",
        "# Get all fraud cases\n",
        "fraud_sampled = fraud.sample(n=nu, random_state=42)\n",
        "\n",
        "# Combine the two datasets\n",
        "balanced_data = pd.concat([non_fraud_sampled, fraud_sampled])\n",
        "\n",
        "# Shuffle the data to mix fraud and non-fraud entries\n",
        "df = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# ✅ Drop rows with missing target\n",
        "df = df.dropna()\n",
        "\n",
        "# ✅ Separate features and target\n",
        "X = df.drop(columns=['isFraud'])\n",
        "y = df['isFraud']\n",
        "\n",
        "# ✅ Encode categorical features\n",
        "cat_cols = X.select_dtypes(include=['object']).columns\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[(\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "X = ct.fit_transform(X)\n",
        "\n",
        "# ✅ Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ✅ Apply SMOTE\n",
        "minority_class_size = y_train.value_counts().min()\n",
        "k = min(5, minority_class_size - 1)\n",
        "\n",
        "sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# ✅ Normalize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# ✅ Build model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.009),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "# ✅ Early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# ✅ Train\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "# ✅ Predict with custom threshold\n",
        "y_prob = model.predict(X_test)\n",
        "y_pred = (y_prob > 0.5).astype(int)\n",
        "\n",
        "# ✅ Classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
